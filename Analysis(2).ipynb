{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe41e893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pysentiment2 as ps\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879e6b0",
   "metadata": {},
   "source": [
    "# 將sentence轉為token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e1fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datapath = []\n",
    "path = os.path.join(os.getcwd(), 'abbrev_news')\n",
    "for dirname in os.listdir(path):#資料夾裡的子資料夾\n",
    "    dir_path = os.listdir(os.path.join(path, dirname))\n",
    "    for file in dir_path:\n",
    "        if \"_company.csv\" in file:\n",
    "            datapath.append(os.path.join(path, dirname, file))\n",
    "print(len(datapath))\n",
    "datapath[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda3315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datapath)):\n",
    "    print(i)\n",
    "    path = datapath[i]\n",
    "    data = pd.read_csv(path)\n",
    "\n",
    "\n",
    "    data.drop_duplicates(subset=['headline'], inplace = True, ignore_index = True)#刪除重複sample\n",
    "    data.drop(columns = [\"link\"], inplace = True)#刪除連結\n",
    "    if 'time' in data.columns:\n",
    "        data.drop(columns = [\"time\"], inplace = True)#後來抓得有些有時間\n",
    "\n",
    "\n",
    "    data[\"head_token\"] = None\n",
    "    data[\"text_token\"] = None\n",
    "\n",
    "\n",
    "    stop_words = stopwords.words('english')#要刪掉的word\n",
    "    exclude = set(string.punctuation)#要刪掉的標點符號\n",
    "    for i in range(0, data.shape[0]):\n",
    "        '''處理 Head'''\n",
    "        #小寫\n",
    "        text = data.iloc[i, 0].lower()\n",
    "        # 移除標點符號\n",
    "        text = ''.join(ch for ch in text if ch not in exclude)\n",
    "        # token 句子\n",
    "        tokens = word_tokenize(text)\n",
    "        # 移除 stopwords\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # 將 token 連成 str, 使用 , 連結\n",
    "        data.iloc[i, -2] = ','.join(x for x in tokens)\n",
    "\n",
    "        '''處理 text'''\n",
    "        #小寫\n",
    "        text = data.iloc[i, 2].lower()\n",
    "        # 移除標點符號\n",
    "        text = ''.join(ch for ch in text if ch not in exclude)\n",
    "        # token 句子 \n",
    "        tokens = word_tokenize(text)\n",
    "        # 移除 stopwords\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        # 將 token 連成 str, 使用 , 連結\n",
    "        data.iloc[i, -1] = ','.join(x for x in tokens)\n",
    "\n",
    "\n",
    "    path = path.replace(\".csv\", \"_b.csv\")\n",
    "    data.to_csv(path, index = False)#save\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cab19b",
   "metadata": {},
   "source": [
    "# 計算各情緒字的個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4851b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = []\n",
    "path = os.path.join(os.getcwd(), 'abbrev_news')\n",
    "for dirname in os.listdir(path):#資料夾裡的子資料夾\n",
    "    dir_path = os.listdir(os.path.join(path, dirname))\n",
    "    for file in dir_path:\n",
    "        if (\"_b.csv\" in file):\n",
    "            datapath.append(os.path.join(path, dirname, file))\n",
    "print(len(datapath))\n",
    "datapath[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25be02fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datapath)):\n",
    "    print(i)\n",
    "    path = datapath[i]\n",
    "    b_data = pd.read_csv(path)\n",
    "\n",
    "\n",
    "    b_data[\"head_pos\"]      = 0\n",
    "    b_data[\"head_neg\"]      = 0\n",
    "    b_data[\"head_neutral\"]  = 0\n",
    "    b_data[\"head_total\"]    = 0\n",
    "    b_data[\"text_pos\"]      = 0\n",
    "    b_data[\"text_neg\"]      = 0\n",
    "    b_data[\"text_neutral\"]  = 0\n",
    "    b_data[\"text_total\"]    = 0\n",
    "\n",
    "\n",
    "    hiv4 = ps.HIV4()\n",
    "    for i in range(0, b_data.shape[0]):\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neutral = 0\n",
    "        for word in b_data[\"head_token\"][i].split(\",\"):#用\",\"把字切割成list\n",
    "            tokens = hiv4.tokenize(word)#先把字放入hiv4.tokenize才能計算分數\n",
    "            score = hiv4.get_score(tokens)#計算分數\n",
    "            if(score[\"Polarity\"] > 0):#正面\n",
    "                pos = pos + 1\n",
    "            elif (score[\"Polarity\"] < 0) :#負面\n",
    "                neg = neg + 1\n",
    "            else:#中性\n",
    "                neutral = neutral + 1\n",
    "        b_data.iloc[i, -8] = pos\n",
    "        b_data.iloc[i, -7] = neg\n",
    "        b_data.iloc[i, -6] = neutral\n",
    "        b_data.iloc[i, -5] = len(b_data[\"head_token\"][i].split(\",\"))#用\",\"把自切割成list後計算長度-->總字數\n",
    "\n",
    "        pos = 0\n",
    "        neg = 0\n",
    "        neutral = 0      \n",
    "        for word in b_data[\"text_token\"][i].split(\",\"):#用\",\"把字切割成list\n",
    "            tokens = hiv4.tokenize(word)#先把字放入hiv4.tokenize才能計算分數\n",
    "            score = hiv4.get_score(tokens)#計算分數\n",
    "            if(score[\"Polarity\"] > 0):#正面\n",
    "                pos = pos + 1\n",
    "            elif (score[\"Polarity\"] < 0) :#負面\n",
    "                neg = neg + 1\n",
    "            else:#中性\n",
    "                neutral = neutral + 1\n",
    "        b_data.iloc[i, -4] = pos\n",
    "        b_data.iloc[i, -3] = neg\n",
    "        b_data.iloc[i, -2] = neutral\n",
    "        b_data.iloc[i, -1] = len(b_data[\"text_token\"][i].split(\",\"))#用\",\"把自切割成list後計算長度-->總字數\n",
    "\n",
    "\n",
    "    path = path.replace(\"_b.csv\", \"_c.csv\")\n",
    "    b_data.to_csv(path, index = False)#save\n",
    "\n",
    "\n",
    "b_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5cf06b",
   "metadata": {},
   "source": [
    "# 計算ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782f4a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = []\n",
    "path = os.path.join(os.getcwd(), 'abbrev_news')\n",
    "for dirname in os.listdir(path):#資料夾裡的子資料夾\n",
    "    dir_path = os.listdir(os.path.join(path, dirname))\n",
    "    for file in dir_path:\n",
    "        if (\"_c.csv\" in file):\n",
    "            datapath.append(os.path.join(path, dirname, file))\n",
    "print(len(datapath))\n",
    "datapath[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986effd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datapath)):\n",
    "    print(i)\n",
    "    path = datapath[i]\n",
    "    c_data = pd.read_csv(path)\n",
    "\n",
    "\n",
    "    aggregation_functions = {'head_pos': 'sum', 'head_neg': 'sum', 'head_neutral': \"sum\",\\\n",
    "                            'head_total': 'sum', 'text_pos': 'sum', 'text_neg': \"sum\",\\\n",
    "                            'text_neutral': 'sum', 'text_total': 'sum',}\n",
    "    #按照日期groupby，並加總\n",
    "    d_data = c_data.groupby(c_data['date']).aggregate(aggregation_functions)\n",
    "    d_data.reset_index(inplace = True)\n",
    "\n",
    "\n",
    "    d_data['head_Pos_ratio']     = None\n",
    "    d_data['head_Neg_ratio']     = None\n",
    "    d_data['head_Netural_ratio'] = None\n",
    "    d_data['head_Polarity']      = None\n",
    "    d_data['head_Subjectivity']  = None\n",
    "    d_data['text_Pos_ratio']     = None\n",
    "    d_data['text_Neg_ratio']     = None\n",
    "    d_data['text_Netural_ratio'] = None\n",
    "    d_data['text_Polarity']      = None\n",
    "    d_data['text_Subjectivity']  = None\n",
    "\n",
    "\n",
    "    for w in range(0, d_data.shape[0]):\n",
    "        if(d_data[\"head_pos\"].iloc[w]+d_data[\"head_neg\"].iloc[w]!=0):#分母不可為0\n",
    "            d_data.iloc[w, 9] = d_data[\"head_pos\"].iloc[w]\\\n",
    "            /(d_data[\"head_pos\"].iloc[w]+d_data[\"head_neg\"].iloc[w])#head_Pos_ratio\n",
    "\n",
    "            d_data.iloc[w, 10] = d_data[\"head_neg\"].iloc[w]\\\n",
    "            /(d_data[\"head_pos\"].iloc[w]+d_data[\"head_neg\"].iloc[w])#head_Neg_ratio\n",
    "\n",
    "            d_data.iloc[w, 12] = (d_data[\"head_pos\"].iloc[w]-d_data[\"head_neg\"].iloc[w])\\\n",
    "            /(d_data[\"head_pos\"].iloc[w]+d_data[\"head_neg\"].iloc[w])#head_Polarity\n",
    "        else:\n",
    "            d_data.iloc[w, 9] = 0 #head_Pos_ratio\n",
    "            d_data.iloc[w, 10] = 0 #head_Neg_ratio        \n",
    "            d_data.iloc[w, 12] = 0 #head_Polarity\n",
    "\n",
    "        if(d_data[\"head_total\"].iloc[w]!=0):#分母不可為0\n",
    "            d_data.iloc[w, 11] = d_data[\"head_neutral\"].iloc[w]/d_data[\"head_total\"].iloc[w]#head_Netural_ratio\n",
    "\n",
    "            d_data.iloc[w, 13] = (d_data[\"head_pos\"].iloc[w]+d_data[\"head_neg\"].iloc[w])/ \\\n",
    "            (d_data[\"head_total\"].iloc[w])#head_Subjectivity\n",
    "        else:\n",
    "            d_data.iloc[w, 11] = 0 #head_Netural_ratio\n",
    "            d_data.iloc[w, 13] = 0 #head_Subjectivity\n",
    "\n",
    "\n",
    "        if(d_data[\"text_pos\"].iloc[w]+d_data[\"text_neg\"].iloc[w]!=0):#分母不可為0\n",
    "            d_data.iloc[w, 14] = d_data[\"text_pos\"].iloc[w]\\\n",
    "            /(d_data[\"text_pos\"].iloc[w]+d_data[\"text_neg\"].iloc[w])#text_Pos_ratio\n",
    "\n",
    "            d_data.iloc[w, 15] = d_data[\"text_neg\"].iloc[w]\\\n",
    "            /(d_data[\"text_pos\"].iloc[w]+d_data[\"text_neg\"].iloc[w])#text_Neg_ratio\n",
    "\n",
    "            d_data.iloc[w, 17] = (d_data[\"text_pos\"].iloc[w]-d_data[\"text_neg\"].iloc[w])\\\n",
    "            /(d_data[\"text_pos\"].iloc[w]+d_data[\"text_neg\"].iloc[w])#text_Polarity     \n",
    "        else:\n",
    "            d_data.iloc[w, 14] = 0 #text_Pos_ratio\n",
    "            d_data.iloc[w, 15] = 0 #text_Neg_ratio\n",
    "            d_data.iloc[w, 17] = 0 #text_Polarity\n",
    "\n",
    "        if(d_data[\"text_total\"].iloc[w] != 0):#分母不可為0\n",
    "            d_data.iloc[w, 16] =  d_data[\"text_neutral\"].iloc[w]/d_data[\"text_total\"].iloc[w]#text_Netural_ratio\n",
    "            d_data.iloc[w, 18] = (d_data[\"text_pos\"].iloc[w]+d_data[\"text_neg\"].iloc[w])\\\n",
    "            /d_data[\"text_total\"].iloc[w]#text_Subjectivity\n",
    "        else:\n",
    "            d_data.iloc[w, 16] = 0 #text_Netural_ratio\n",
    "            d_data.iloc[w, 18] = 0 #text_Subjectivity\n",
    "\n",
    "\n",
    "    path = path.replace(\"_c.csv\", \"_d.csv\")\n",
    "    d_data.to_csv(path, index = False)#save\n",
    "\n",
    "\n",
    "d_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfd3cda",
   "metadata": {},
   "source": [
    "# 透過standarization計算ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d509d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = []\n",
    "path = os.path.join(os.getcwd(), 'abbrev_news')\n",
    "for dirname in os.listdir(path):#資料夾裡的子資料夾\n",
    "    dir_path = os.listdir(os.path.join(path, dirname))\n",
    "    for file in dir_path:\n",
    "        if (\"_d.csv\" in file):\n",
    "            datapath.append(os.path.join(path, dirname, file))\n",
    "print(len(datapath))\n",
    "datapath[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(datapath)):\n",
    "    print(i)\n",
    "    path = datapath[i]\n",
    "    e_data = pd.read_csv(path)\n",
    "\n",
    "\n",
    "    # standarization = (X - mean) / std\n",
    "    e_data['head_sPos_ratio']     =\\\n",
    "    (e_data[\"head_Pos_ratio\"]-e_data[\"head_Pos_ratio\"].mean())/e_data[\"head_Pos_ratio\"].std()\n",
    "\n",
    "    e_data['head_sNeg_ratio']     =\\\n",
    "    (e_data[\"head_Neg_ratio\"]-e_data[\"head_Neg_ratio\"].mean())/e_data[\"head_Neg_ratio\"].std()\n",
    "\n",
    "    e_data['head_sNetural_ratio'] =\\\n",
    "    (e_data[\"head_Netural_ratio\"]-e_data[\"head_Netural_ratio\"].mean())/e_data[\"head_Netural_ratio\"].std()\n",
    "\n",
    "    e_data['head_sPolarity']      =\\\n",
    "    (e_data[\"head_Polarity\"]-e_data[\"head_Polarity\"].mean())/e_data[\"head_Polarity\"].std()\n",
    "\n",
    "    e_data['head_sSubjectivity']  =\\\n",
    "    (e_data[\"head_Subjectivity\"]-e_data[\"head_Subjectivity\"].mean())/e_data[\"head_Subjectivity\"].std()\n",
    "\n",
    "    e_data['text_sPos_ratio']     =\\\n",
    "    (e_data[\"text_Pos_ratio\"]-e_data[\"text_Pos_ratio\"].mean())/e_data[\"text_Pos_ratio\"].std()\n",
    "\n",
    "    e_data['text_sNeg_ratio']     =\\\n",
    "    (e_data[\"text_Neg_ratio\"]-e_data[\"text_Neg_ratio\"].mean())/e_data[\"text_Neg_ratio\"].std()\n",
    "\n",
    "    e_data['text_sNetural_ratio'] =\\\n",
    "    (e_data[\"text_Netural_ratio\"]-e_data[\"text_Netural_ratio\"].mean())/e_data[\"text_Netural_ratio\"].std()\n",
    "\n",
    "    e_data['text_sPolarity']      =\\\n",
    "    (e_data[\"text_Polarity\"]-e_data[\"text_Polarity\"].mean())/e_data[\"text_Polarity\"].std()\n",
    "\n",
    "    e_data['text_sSubjectivity']  =\\\n",
    "    (e_data[\"text_Subjectivity\"]-e_data[\"text_Subjectivity\"].mean())/e_data[\"text_Subjectivity\"].std()\n",
    "    \n",
    "    \n",
    "    #做standarization時，當std=0在分母，結果會是nan，須補0\n",
    "    e_data.fillna(0, inplace = True)\n",
    "\n",
    "    \n",
    "    path = path.replace(\"_d.csv\", \"_e.csv\")\n",
    "    e_data.to_csv(path, index = False)#save\n",
    "\n",
    "e_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b74b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
