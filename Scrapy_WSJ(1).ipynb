{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time \n",
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(\"C:/Users/i7-870/chromedriver.exe\")\n",
    "#driver = webdriver.Chrome(\"C:/Users/User/Documents/Julie/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.wsj.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sign in\n",
    "e = driver.find_element_by_css_selector('#root > div > div > div > div.WSJTheme--margin-bottom--2-lor3Ur.styles--margin-bottom--1qLtxtgQ > header > div.style--login-buttons--3iPE-lSo.style--clearfix--P-MgmEGt > a:nth-child(2)')\n",
    "e.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#輸入email\n",
    "q = driver.find_element_by_name('username')\n",
    "q.send_keys('kenneth.wy.lee21@gmail.com')\n",
    "\n",
    "#輸入密碼\n",
    "q = driver.find_element_by_name('password')\n",
    "q.send_keys('qwer0987')\n",
    "\n",
    "#sign in\n",
    "e = driver.find_element_by_css_selector('#basic-login > div:nth-child(1) > form > div > div:nth-child(6) > div.sign-in.hide-if-one-time-linking > button')\n",
    "e.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Abbreviate company name\n",
    "sp_500 = pd.read_csv('WSJ S&P 500 成分股.csv')\n",
    "\n",
    "#[\"Co.\", \"Corp.\", \"Cos.\"]\n",
    "keyword = []\n",
    "for i in sp_500[\"Company\"]:\n",
    "    company = i.replace(\" Cl A\", \"\").replace(\" Cl B\", \"\").replace(\" Cl C\", \"\")\\\n",
    "                .replace(\" Series A\",\"\").replace(\" Series C\",\"\")\\\n",
    "                .replace(\" Inc.\", \"\")\\\n",
    "                .replace(\" PLC\", \"\")\\\n",
    "                .replace(\" Ltd.\", \"\")\\\n",
    "                .replace(\".com\", \"\")\\\n",
    "                .replace(\" N.A.\", \"\")\\\n",
    "                .replace(\" N.V.\", \"\")\n",
    "    keyword.append(company)\n",
    "sp_500[\"Company\"] = keyword\n",
    "sp_500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化query，搜尋時使用\n",
    "sp_500[\"query1\"] = sp_500[\"Company\"].apply(lambda x: x.replace(\" \", \"%20\")\\\n",
    "                                                   .replace(\"'\", \"%27\")\\\n",
    "                                                   .replace(\"&\", \"%26\"))\n",
    "sp_500[\"query1\"] = sp_500[\"query1\"].apply(lambda x: \"%22\" + x + \"%22\")\n",
    "\n",
    "sp_500[\"query2\"] = sp_500[\"Symbol\"].apply(lambda x: \"%22\" + x + \"%22\")\n",
    "sp_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):#len(sp_500)):\n",
    "    \n",
    "    #建立新的資料夾，名稱是company\n",
    "    dirpath = \"C:\\\\Users\\\\i7-870\\\\Documents\\\\Python\\\\news\\\\WSJ\\\\news\\\\\"\n",
    "    path = os.path.join(dirpath, sp_500[\"Company\"][i])\n",
    "\n",
    "    if not os.path.exists(path):#如果沒有該路徑\n",
    "        os.makedirs(path)#就建立該路徑\n",
    "    #print(\"創建資料夾:\", path)\n",
    "    \n",
    "    #連接url\n",
    "    url0 = \"https://www.wsj.com/search?query={query}&isToggleOn=true&operator=AND&sort=date-desc&duration=4y&startDate=2005%2F05%2F05&endDate=2021%2F06%2F08&source=%2Cwsjpro%2Cwsjie\"\\\n",
    "    .format(query = sp_500[\"query1\"][i])\n",
    "    \n",
    "    driver.get(url0)\n",
    "    \n",
    "    time.sleep(random.uniform(3,6))\n",
    "    \n",
    "    \n",
    "    '''抓出搜尋頁數'''\n",
    "    #查看該公司的搜尋結果有幾頁\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    try:\n",
    "        try:\n",
    "            #抓頁數\n",
    "            pages = soup.find(\"div\", \"WSJTheme--SearchResultPagination--2_RDsqbb\").find_all(\"span\")[-1].text\n",
    "            pages = pages.split(' ')[1]\n",
    "            print(i, sp_500[\"Company\"][i], \" Pages: \", pages)\n",
    "        except:\n",
    "            if soup.find(\"title\").text == \"Page Not Found\":\n",
    "                time.sleep(random.uniform(150, 200))\n",
    "            \n",
    "            #有的時候driver.get()會連不上，因此放外面，方便同時給if或連不上時使用\n",
    "            driver.get(url0)\n",
    "            time.sleep(random.uniform(3,6))  \n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            \n",
    "            #抓頁數\n",
    "            pages = soup.find(\"div\", \"WSJTheme--SearchResultPagination--2_RDsqbb\").find_all(\"span\")[-1].text\n",
    "            pages = pages.split(' ')[1]\n",
    "            print(i, sp_500[\"Company\"][i], \" Pages: \", pages)\n",
    "    except:\n",
    "        #no result，跳下一家公司\n",
    "        if soup.find(\"div\", \"WSJTheme--no-result--mNcujucR\").text == \"No articles or videos have been found.\":\n",
    "            print(i, sp_500[\"Company\"][i], \" no result\")\n",
    "            continue\n",
    "    \n",
    "    \n",
    "    '''抓出文章'''\n",
    "    #依照頁數逐頁抓取\n",
    "    df = []\n",
    "    for page in range(1, int(pages)+1):\n",
    "        \n",
    "        #連接加了頁數的url\n",
    "        url = url0 + \"&page=\" + str(page)\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(random.uniform(4,6))\n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #如果抓不到article，就等幾分鐘再重新連接並返回url檢查\n",
    "        while True:\n",
    "            ''''''\n",
    "            elements = soup.find_all(\"article\", \"WSJTheme--story--XB4V2mLz WSJTheme--overflow-visible--3OB31tWq WSJTheme--border-bottom--s4hYCt0s\")\n",
    "            ''''''\n",
    "            if elements == []: #如果沒有news         \n",
    "                print(url)\n",
    "                time.sleep(random.uniform(150, 200))\n",
    "                driver.get(url)#重新連接\n",
    "                time.sleep(random.uniform(4,6))\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                continue\n",
    "            else:#如果有news\n",
    "                break\n",
    "        \n",
    "        \n",
    "        for ele in elements:\n",
    "            headline = ele.find(\"span\", \"WSJTheme--headlineText--He1ANr9C\").text\n",
    "            link = ele.find(\"a\")['href']\n",
    "            time_ = ele.find(\"div\", \"WSJTheme--timestamp--2zjbypGD\").text\n",
    "            \n",
    "            if time_ == None:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                category = ele.find(\"li\", \"WSJTheme--type--3JhCic1c\").text\n",
    "            except:\n",
    "                category = \"\"\n",
    "            \n",
    "            df.append({\n",
    "                \"Headline\": headline,\n",
    "                \"Links\": link,\n",
    "                \"Time\": time_,\n",
    "                \"Category\": category,\n",
    "            })\n",
    "        if time_ == None:\n",
    "            break\n",
    "    if time_ == None:\n",
    "        break       \n",
    "    #save\n",
    "    pd.DataFrame(df).to_csv(os.path.join(path, sp_500[\"Company\"][i]) + \"_company_url.csv\",index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath = []\n",
    "path = os.path.join(os.getcwd(), 'news')\n",
    "print(path)\n",
    "for dirname in os.listdir(path):#資料夾裡的子資料夾\n",
    "    dir_path = os.listdir(os.path.join(path, dirname))\n",
    "     \n",
    "    for filename in dir_path:\n",
    "        if \"_company_url.csv\" in filename:\n",
    "            datapath.append(os.path.join(path, dirname, filename))\n",
    "print(len(datapath))\n",
    "datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(datapath)):\n",
    "    print(\"-----------------------------------------------------------------------------------\")\n",
    "    print(i, datapath[i])\n",
    "    data = pd.read_csv(datapath[i])#讀取公司相關新聞的url\n",
    "    \n",
    "    df = []\n",
    "    erro=[]\n",
    "    for url in range(len(data[\"Links\"])):#依序抓出url\n",
    "        driver.get(data[\"Links\"][url])\n",
    "        time.sleep(2) \n",
    "        \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')#讀取網站的html\n",
    "        \n",
    "        \n",
    "        #確認是WSJ Articles還是WSJ PRO Articles\n",
    "        source = \"articles\"\n",
    "        if soup.find(\"a\", \"style--logo-1t5fLHT-VmzJTtPNZR0C0P\"):\n",
    "            source = \"pro articles\"\n",
    "            \n",
    "            \n",
    "        '''確認article是不是我們要的類型'''\n",
    "        #如果是出售報價不是news，就跳過\n",
    "        if soup.find(\"hr\", \"web-ui-slideshow-hr\"):\n",
    "            print(\"----\", url, \"Sales\", data[\"Links\"][url])\n",
    "            continue\n",
    "\n",
    "        #如果是Noted，就跳過\n",
    "        if soup.find(\"a\", \"style--section-logo-1OPM_iWTXp3KPZjnR-sGBU style--noted-3D4iJQdRyI-61o3KCcIwAB style--section-logo-37slLHd9h2vhlXUDQBDu6n\"):\n",
    "            print(\"----\", url, \"Noted\", data[\"Links\"][url])\n",
    "            continue\n",
    "        \n",
    "        '''確定有進入文章'''\n",
    "        #連接失敗才執行\n",
    "        count = 0\n",
    "        while True:#連線太多次會失敗\n",
    "            try :#連接成功\n",
    "                try:\n",
    "                    h1 = soup.find(\"h1\", \"wsj-article-headline\").text\n",
    "                    break\n",
    "                except:\n",
    "                    try:\n",
    "                        h1 = soup.find(\"h1\", \"bigTop__hed\").text\n",
    "                        break\n",
    "                    except:\n",
    "                        h1 = soup.find(\"h1\", \"headline\").text\n",
    "                        break\n",
    "            except:#如果連接失敗\n",
    "                count += 1\n",
    "                if count > 5:#如過持續重新載入失敗，代表網頁無回應\n",
    "                    erro.append([datapath[i], url, \"unconnect\", data[\"Links\"][url]])\n",
    "                    break\n",
    "                print(url, \"unconnect\", data[\"Links\"][url])\n",
    "                time.sleep(200)#等幾分鐘\n",
    "                driver.get(data[\"Links\"][url])#重新連接\n",
    "                time.sleep(2)\n",
    "                soup = BeautifulSoup(driver.page_source, 'html.parser')#讀取網站的html\n",
    "                continue\n",
    "        \n",
    "        if count > 5:#如過持續重新載入失敗，代表網頁無回應\n",
    "            break\n",
    "            \n",
    "            \n",
    "        '''text'''\n",
    "        text = str()\n",
    "        try:\n",
    "            try:\n",
    "                element = soup.find(\"div\", \"article-content\").find_all(\"p\")\n",
    "                for ele in element:\n",
    "                    text += ele.text.replace(\"\\n\", '').replace(\"  \", '')\n",
    "            except:\n",
    "                try:\n",
    "                    element = soup.find(\"div\", \"nc-exp-article\").find_all(\"p\")\n",
    "                    for ele in element:\n",
    "                        text += ele.text.replace(\"\\n\", '').replace(\"  \", '')\n",
    "                except:\n",
    "                    try:\n",
    "                        element = soup.find_all(\"h3\", \"section-title\")\n",
    "                        for ele in element:\n",
    "                            text += ele.text.replace(\"\\n\", '').replace(\"  \", '')\n",
    "\n",
    "                        if text == \"\":\n",
    "                            raise\n",
    "                    except:\n",
    "                        #pro article\n",
    "                        element = soup.find(\"div\", \"wsj-snippet-body\").find_all(\"p\")\n",
    "                        for ele in element:\n",
    "                            text += ele.text.replace(\"\\n\", '').replace(\"  \", '')\n",
    "        except:\n",
    "            print(url, \"text\", data[\"Links\"][url])\n",
    "            erro.append([datapath[i], url, \"text\", data[\"Links\"][url]])\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        #將資料存入DataFrame\n",
    "        df.append({\n",
    "            \"headline\": data[\"Headline\"][url],\n",
    "            \"time\": data[\"Time\"][url],\n",
    "            \"link\": data[\"Links\"][url],\n",
    "            \"text\": text,\n",
    "            \"category\": data[\"Category\"][url],\n",
    "            \"source\": source,\n",
    "        })\n",
    "        \n",
    "        \n",
    "    #save corpus\n",
    "    path = datapath[i].replace(\"_url\", \"\")#刪除url的部分\n",
    "    pd.DataFrame(df).to_csv(path, index = False)#save\n",
    "    \n",
    "    \n",
    "    #save erro\n",
    "    erro0 = pd.read_csv(\"erro.csv\")\n",
    "    erro = pd.DataFrame(erro, columns=[\"file\", \"index\",'problem', 'link'])\n",
    "    erro = erro0.append(erro, ignore_index = True)\n",
    "    erro.to_csv('erro.csv',index = False)#save"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
